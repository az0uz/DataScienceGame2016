{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import xgboost as xgb\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "############## HYPEROPT ########################\n",
    "from hyperopt import hp\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../data/X_train.csv')\n",
    "all_columns = train_set.columns.tolist\n",
    "\n",
    "## add response\n",
    "Y_train = pd.read_csv('../data/Y_train.csv')\n",
    "train_set['Converted'] = Y_train.Converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_kfold = pd.read_csv('../data/fold_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_var = ['SCID', 'FirstDriverDrivingLicenseNumberY', 'FirstDriverMaritalStatus', 'CarUsageId', 'CarParkingTypeId', 'FirstDriverDrivingLicenceType', 'CarDrivingEntitlement', 'CarMakeId', 'NameOfPolicyProduct', 'AffinityCodeId']\n",
    "###'SelectedPackage' is constant\n",
    "## 'SCID' to add eventually\n",
    "already_binary = ['CoverIsNoClaimDiscountSelected',  'IsPolicyholderAHomeowner']\n",
    "\n",
    "to_0_1 = ['CarFuelId', 'CarTransmissionId']\n",
    "train_set[to_0_1] = train_set[to_0_1].apply(lambda x : x - 1)\n",
    "\n",
    "row_id = [u'Unnamed: 0']\n",
    "\n",
    "continuous_var = ['CarAnnualMileage', 'FirstDriverAge', 'CarInsuredValue', 'CarAge', 'VoluntaryExcess', 'PolicyHolderNoClaimDiscountYears', 'PolicyHolderResidencyArea', 'AllDriversNbConvictions', 'RatedDriverNumber','SocioDemographicId', 'DaysSinceCarPurchase']\n",
    "\n",
    "continuous_var = continuous_var + already_binary + to_0_1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out_var = ['ReceivedDateTime', 'TodayDate']\n",
    "\n",
    "\n",
    "dunno_var = ['CustomerMD5Key']\n",
    "\n",
    "response = \"Converted\"\n",
    "\n",
    "\n",
    "total_var = categorical_var + continuous_var + [response]\n",
    "\n",
    "train_set = train_set[total_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### FILL NA\n",
    "\n",
    "train_set['IsPolicyholderAHomeowner'] = train_set['IsPolicyholderAHomeowner'].fillna(-1)\n",
    "train_set['SCID'] = train_set['SCID'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simplify_scid(x) :\n",
    "    if x == 'B01851' :\n",
    "        return 0\n",
    "    elif x == 'A04402' :\n",
    "        return 1\n",
    "    elif x == 'A10099' :\n",
    "        return 2\n",
    "    elif x == 'B02196' :\n",
    "        return 3\n",
    "    elif x == 'A08213' :\n",
    "        return 4\n",
    "    elif x == 'A03440' :\n",
    "        return 5\n",
    "    elif x == 'A04955' :\n",
    "        return 6\n",
    "    elif x == 'A09963' :\n",
    "        return 7\n",
    "    elif x == 'B01604' :\n",
    "        return 8\n",
    "    elif x == 'A06439' :\n",
    "        return 9\n",
    "    else :\n",
    "        return 10\n",
    "    \n",
    "def simplify_affinity(x) :\n",
    "    \n",
    "    if x == 0 :\n",
    "        return 0\n",
    "    elif x == 31 :\n",
    "        return 1\n",
    "    elif x==39 :\n",
    "        return 2\n",
    "    else :\n",
    "        return 3\n",
    "    \n",
    "def simplify_DrivingLicense(x) :\n",
    "    if (x % 10) in [0,1] :\n",
    "        return 0\n",
    "    elif (x % 10) in [5,6] :\n",
    "        return 1\n",
    "    else :\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_set.SCID = train_set.SCID.apply(simplify_scid)\n",
    "train_set.AffinityCodeId = train_set.AffinityCodeId.apply(simplify_affinity)\n",
    "train_set.FirstDriverDrivingLicenseNumberY = train_set.FirstDriverDrivingLicenseNumberY.apply(simplify_DrivingLicense)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Features Antoine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmented_features_antoine = pd.read_csv('./AugmentedFeaturesTrain.zip')\n",
    "feat_name = [ u'VariationCarUsageId',\n",
    "       u'VariationFirstDriverDrivingLicenseNumberY',\n",
    "       u'VariationCoverIsNoClaimDiscountSelected',\n",
    "       u'VariationIsPolicyholderAHomeowner', u'isMostFrequentCarMakeId',\n",
    "       u'VariationVoluntaryExcess',\n",
    "       u'VariationPolicyHolderNoClaimDiscountYears', u'VariationCarAge',\n",
    "       u'VariationCarMakeId', u'rowNumberForUser', u'VariationCarInsuredValue',\n",
    "       u'VariationCarParkingTypeId',\n",
    "       u'isMostFrequentFirstDriverDrivingLicenseNumberY',\n",
    "       u'VariationCarAnnualMileage', u'driverPerCar',\n",
    "       u'VariationDaysSinceCarPurchase', u'VariationCarDrivingEntitlement']\n",
    "\n",
    "continuous_var = continuous_var + feat_name\n",
    "\n",
    "train_set = pd.concat([train_set, augmented_features_antoine[feat_name]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Features Alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmented_features_alex = pd.read_csv('./lda_features_5_train_topics_df.csv')\n",
    "feature_name_alex = augmented_features_alex.columns.tolist()\n",
    "\n",
    "continuous_var = continuous_var + feature_name_alex\n",
    "\n",
    "train_set = pd.concat([train_set, augmented_features_alex[feature_name_alex]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at categorical factors...\n",
      "The threshold is set to 50\n",
      "... Done!\n",
      "\n",
      "Creating hash tables...\n",
      "... Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#####################################################\n",
    "########## Encoding functions #######################\n",
    "def OHEvsHash(df, factors, thresh ) :\n",
    "\t\"\"\"\n",
    "\tdf : dataset,\n",
    "\tfactors : categorical factors,\n",
    "\tthresh : when a factor should be considered to big for OHE\n",
    "\t\"\"\"\n",
    "\tOHE_factors = []\n",
    "\tHash_factors = []\n",
    "\tfor f in factors :\n",
    "\t\tnb_categories = len(df[f].unique())\n",
    "\t\tif nb_categories > thresh :\n",
    "\t\t\tHash_factors.append(f)\n",
    "\t\telse :\n",
    "\t\t\tOHE_factors.append(f)\n",
    "\treturn OHE_factors, Hash_factors\n",
    "\n",
    "def encoder (b, encoder) :\n",
    "\ttry :\n",
    "\t\treturn float(encoder.transform(b))\n",
    "\texcept :\n",
    "\t\treturn -1\n",
    "\n",
    "def create_hash_dict(df,Hash_factors) :\n",
    "    encoders = {}\n",
    "    for f in Hash_factors:\n",
    "        dic_aux = {}\n",
    "        for i,e in enumerate(df[f].unique()) :\n",
    "            dic_aux[e]=i            \n",
    "        encoders[f] = dic_aux\n",
    "    return encoders\n",
    "#####################################################\n",
    "\n",
    "\n",
    "print 'Looking at categorical factors...'\t\n",
    "thresh = 50\n",
    "print \"The threshold is set to %d\"%(thresh)\n",
    "OHE_factors, Hash_factors = OHEvsHash(train_set, categorical_var, thresh)\n",
    "print '... Done!\\n'\n",
    "\n",
    "print 'Creating hash tables...'\n",
    "Hashing_dict =create_hash_dict(train_set,Hash_factors)\n",
    "print '... Done!\\n'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_training_data( df, continuous_factors, OHE_factors, Hashing_dict, Hash_factors,  response ) :\n",
    "    \n",
    "    final_factors = []\n",
    "    res_data = pd.DataFrame()\n",
    "\t\n",
    "    print 'Creating Dummies...'\n",
    "    for factor in OHE_factors :\n",
    "        df[factor].fillna('null', inplace = True)\n",
    "        try :\n",
    "            print factor \n",
    "            res_data= pd.concat([res_data, pd.get_dummies(df[factor], prefix = factor)], axis = 1)\n",
    "            final_factors.extend([str(factor).encode('utf8')+'_'+str(x).encode('utf8') for x in list(set(df[factor]))])\n",
    "        except :\n",
    "            print repr(factor.decode('utf8'))\n",
    "            raise \n",
    "    print '...Done for Dummies!\\n'\n",
    "    \n",
    "    print 'Hashing Factors...'\n",
    "    for f in Hash_factors :\n",
    "        print f\n",
    "        res_data[f] = df[f].map(lambda x : Hashing_dict[f][x])\n",
    "        final_factors.append(f)\n",
    "    print \"...Done!\\n\"\n",
    "\t\n",
    "    print 'Continuous Factors...'\n",
    "    final_factors.extend(continuous_factors)\n",
    "    res_data =pd.concat([res_data,df[continuous_factors] ], axis = 1)\n",
    "    #res_data.fillna(-1, inplace = True)\n",
    "    #res_data.replace('null',-1, inplace = True)\n",
    "    #res_data.replace('?',-1, inplace = True)\n",
    "    res_data = res_data[final_factors]\n",
    "    print \"...Done for continuous factors!\\n\"\n",
    "    return (np.matrix(res_data.values), np.array(df[response]).reshape(-1), final_factors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dummies...\n",
      "SCID\n",
      "FirstDriverDrivingLicenseNumberY\n",
      "FirstDriverMaritalStatus\n",
      "CarUsageId\n",
      "CarParkingTypeId\n",
      "FirstDriverDrivingLicenceType\n",
      "CarDrivingEntitlement\n",
      "CarMakeId\n",
      "NameOfPolicyProduct\n",
      "AffinityCodeId\n",
      "...Done for Dummies!\n",
      "\n",
      "Hashing Factors...\n",
      "...Done!\n",
      "\n",
      "Continuous Factors...\n",
      "...Done for continuous factors!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selected_features = pickle.load(open('./Results/selected_featv6.pic', 'rb'))\n",
    "train_matrix, Y, final_factors = preprocess_training_data(train_set, continuous_var, OHE_factors, Hashing_dict, Hash_factors,  response )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_check, y_train, y_check = train_test_split(train_matrix, Y,  test_size = 0.25, random_state = 42)\n",
    "\n",
    "fold_value = 0\n",
    "\n",
    "\n",
    "X_train = train_matrix[np.array([split_kfold.fold!=fold_value]).reshape(-1),:]\n",
    "y_train = Y[np.array([split_kfold.fold!=fold_value]).reshape(-1)]\n",
    "X_check = train_matrix[np.array([split_kfold.fold==fold_value]).reshape(-1),:]\n",
    "y_check = Y[np.array([split_kfold.fold==fold_value]).reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_wneg = (Y_train.Converted == 0).sum()\n",
    "sum_wpos = (Y_train.Converted == 1).sum()\n",
    "\n",
    "params = {\n",
    "\t\t\t 'n_estimators' : 2500,\n",
    "             'eta' : 0.1, #0.1\n",
    "             'max_depth' : 9,\n",
    "             'min_child_weight' : 3,#50\n",
    "             'subsample' :  0.85,\n",
    "             'colsample_bytree' :0.75,\n",
    "\t\t\t \"lambda\" : 10,#100\n",
    "\t\t\t \"alpha\" :3,#100\n",
    "             \"gamma\" : 0.2,\n",
    "             'objective': \"binary:logistic\",\n",
    "             'eval_metric' : \"logloss\",\n",
    "             'max_delta_step':0,\n",
    "             'nthread' : 14,\n",
    "             'silent' : 1,\n",
    "             'scale_pos_weight' : sum_wneg/float(sum_wpos),\n",
    "\t\t\t \"booster\" : \"gbtree\",\n",
    "\t\t\t \"seed\" : 43\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'eval_metric': 'logloss', 'scale_pos_weight': 127.25189427312775, 'max_delta_step': 0, 'nthread': 14, 'min_child_weight': 3, 'n_estimators': 2500, 'subsample': 0.85, 'eta': 0.1, 'objective': 'binary:logistic', 'alpha': 3, 'booster': 'gbtree', 'seed': 43, 'max_depth': 9, 'gamma': 0.2, 'lambda': 10}\n",
      "[0]\ttrain-logloss:0.630886\teval-logloss:0.631441\n",
      "Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-logloss hasn't improved in 20 rounds.\n",
      "[10]\ttrain-logloss:0.204908\teval-logloss:0.20539\n",
      "[20]\ttrain-logloss:0.086535\teval-logloss:0.086941\n",
      "[30]\ttrain-logloss:0.042528\teval-logloss:0.042843\n",
      "[40]\ttrain-logloss:0.026315\teval-logloss:0.026607\n",
      "[50]\ttrain-logloss:0.018658\teval-logloss:0.018961\n",
      "[60]\ttrain-logloss:0.015524\teval-logloss:0.015876\n",
      "[70]\ttrain-logloss:0.01369\teval-logloss:0.014157\n",
      "[80]\ttrain-logloss:0.012555\teval-logloss:0.013084\n",
      "[90]\ttrain-logloss:0.011773\teval-logloss:0.01238\n",
      "[100]\ttrain-logloss:0.011213\teval-logloss:0.011881\n",
      "[110]\ttrain-logloss:0.010737\teval-logloss:0.011483\n",
      "[120]\ttrain-logloss:0.010268\teval-logloss:0.011103\n",
      "[130]\ttrain-logloss:0.009801\teval-logloss:0.01073\n",
      "[140]\ttrain-logloss:0.009443\teval-logloss:0.010445\n",
      "[150]\ttrain-logloss:0.00908\teval-logloss:0.010155\n",
      "[160]\ttrain-logloss:0.008668\teval-logloss:0.00984\n",
      "[170]\ttrain-logloss:0.008398\teval-logloss:0.009654\n",
      "[180]\ttrain-logloss:0.008116\teval-logloss:0.009458\n",
      "[190]\ttrain-logloss:0.007823\teval-logloss:0.009233\n",
      "[200]\ttrain-logloss:0.007598\teval-logloss:0.009065\n",
      "[210]\ttrain-logloss:0.007315\teval-logloss:0.00887\n",
      "[220]\ttrain-logloss:0.007156\teval-logloss:0.008761\n",
      "[230]\ttrain-logloss:0.006947\teval-logloss:0.008638\n",
      "[240]\ttrain-logloss:0.006753\teval-logloss:0.008519\n",
      "[250]\ttrain-logloss:0.006611\teval-logloss:0.008424\n",
      "[260]\ttrain-logloss:0.006441\teval-logloss:0.008321\n",
      "[270]\ttrain-logloss:0.006241\teval-logloss:0.008207\n",
      "[280]\ttrain-logloss:0.006029\teval-logloss:0.008089\n",
      "[290]\ttrain-logloss:0.005872\teval-logloss:0.00801\n",
      "[300]\ttrain-logloss:0.005708\teval-logloss:0.007909\n",
      "[310]\ttrain-logloss:0.005595\teval-logloss:0.007849\n",
      "[320]\ttrain-logloss:0.005456\teval-logloss:0.007778\n",
      "[330]\ttrain-logloss:0.005314\teval-logloss:0.007713\n",
      "[340]\ttrain-logloss:0.005173\teval-logloss:0.007625\n",
      "[350]\ttrain-logloss:0.005053\teval-logloss:0.007576\n",
      "[360]\ttrain-logloss:0.004932\teval-logloss:0.007522\n",
      "[370]\ttrain-logloss:0.004789\teval-logloss:0.00745\n",
      "[380]\ttrain-logloss:0.004682\teval-logloss:0.007402\n",
      "[390]\ttrain-logloss:0.004582\teval-logloss:0.007354\n",
      "[400]\ttrain-logloss:0.004476\teval-logloss:0.007301\n",
      "[410]\ttrain-logloss:0.004369\teval-logloss:0.007257\n",
      "[420]\ttrain-logloss:0.004257\teval-logloss:0.0072\n",
      "[430]\ttrain-logloss:0.004178\teval-logloss:0.007168\n",
      "[440]\ttrain-logloss:0.004069\teval-logloss:0.007125\n",
      "[450]\ttrain-logloss:0.003951\teval-logloss:0.007081\n",
      "[460]\ttrain-logloss:0.003846\teval-logloss:0.007046\n",
      "[470]\ttrain-logloss:0.003765\teval-logloss:0.007025\n",
      "[480]\ttrain-logloss:0.003691\teval-logloss:0.006999\n",
      "[490]\ttrain-logloss:0.003606\teval-logloss:0.006967\n",
      "[500]\ttrain-logloss:0.003528\teval-logloss:0.006948\n",
      "[510]\ttrain-logloss:0.003463\teval-logloss:0.006933\n",
      "[520]\ttrain-logloss:0.003375\teval-logloss:0.006907\n",
      "[530]\ttrain-logloss:0.003296\teval-logloss:0.00688\n",
      "[540]\ttrain-logloss:0.00321\teval-logloss:0.006852\n",
      "[550]\ttrain-logloss:0.003137\teval-logloss:0.006831\n",
      "[560]\ttrain-logloss:0.003071\teval-logloss:0.006809\n",
      "[570]\ttrain-logloss:0.002991\teval-logloss:0.006794\n",
      "[580]\ttrain-logloss:0.00291\teval-logloss:0.006776\n",
      "[590]\ttrain-logloss:0.002846\teval-logloss:0.006755\n",
      "[600]\ttrain-logloss:0.00278\teval-logloss:0.006745\n",
      "[610]\ttrain-logloss:0.002719\teval-logloss:0.006727\n",
      "[620]\ttrain-logloss:0.002654\teval-logloss:0.006706\n",
      "[630]\ttrain-logloss:0.002586\teval-logloss:0.006691\n",
      "[640]\ttrain-logloss:0.002514\teval-logloss:0.006678\n",
      "[650]\ttrain-logloss:0.002458\teval-logloss:0.006668\n",
      "[660]\ttrain-logloss:0.002395\teval-logloss:0.006655\n",
      "[670]\ttrain-logloss:0.002336\teval-logloss:0.006643\n",
      "[680]\ttrain-logloss:0.002291\teval-logloss:0.006641\n",
      "[690]\ttrain-logloss:0.002227\teval-logloss:0.006632\n",
      "[700]\ttrain-logloss:0.00218\teval-logloss:0.006625\n",
      "[710]\ttrain-logloss:0.002121\teval-logloss:0.006606\n",
      "[720]\ttrain-logloss:0.002061\teval-logloss:0.006599\n",
      "[730]\ttrain-logloss:0.002011\teval-logloss:0.006592\n",
      "[740]\ttrain-logloss:0.001961\teval-logloss:0.006582\n",
      "[750]\ttrain-logloss:0.001916\teval-logloss:0.006576\n",
      "[760]\ttrain-logloss:0.001879\teval-logloss:0.006573\n",
      "[770]\ttrain-logloss:0.001838\teval-logloss:0.006565\n",
      "[780]\ttrain-logloss:0.001801\teval-logloss:0.006568\n",
      "[790]\ttrain-logloss:0.00176\teval-logloss:0.006559\n",
      "[800]\ttrain-logloss:0.001721\teval-logloss:0.006552\n",
      "[810]\ttrain-logloss:0.001673\teval-logloss:0.006549\n",
      "[820]\ttrain-logloss:0.001633\teval-logloss:0.006538\n",
      "[830]\ttrain-logloss:0.001594\teval-logloss:0.006546\n",
      "[840]\ttrain-logloss:0.001556\teval-logloss:0.006537\n",
      "Stopping. Best iteration:\n",
      "[821]\ttrain-logloss:0.001629\teval-logloss:0.006537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Training with params : \"\n",
    "print params\n",
    "num_round = int(params['n_estimators'])\n",
    "del params['n_estimators']\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_check, label=y_check)\n",
    "watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "\n",
    "\n",
    "model = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=20,  verbose_eval=10)  #feval=logloss,\n",
    "predictions = model.predict(dvalid)\n",
    "train_pred = model.predict(dtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "def save_model( file_name, model ,continuous_factors, OHE_factors , Hashing_dict, Hash_factors,  training_factors ) :\n",
    "    dico = {\"model\" : model, 'continuous_factors' : continuous_factors, 'OHE_factors' :  OHE_factors, \"Hashing_dict\" : Hashing_dict, \"Hash_factors\" : Hash_factors, 'training_factors' : training_factors}\n",
    "    pickle.dump(dico, open( file_name, \"wb\" ) )\n",
    "    print 'Model saved!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "file_path = './Results/model_v7_fold0.pic'\n",
    "save_model(file_path, model, continuous_var, OHE_factors, Hashing_dict, Hash_factors, final_factors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
