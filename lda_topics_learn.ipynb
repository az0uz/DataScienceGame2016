{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"gensim.models.word2vec\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "#get_ipython().magic(u'matplotlib inline')\n",
    "import cPickle\n",
    "from multiprocessing import Pool\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('X_train.csv')\n",
    "Y_train=pd.read_csv('Y_train.csv')\n",
    "\n",
    "df=X_train\n",
    "\n",
    "(lenX,lenY)=np.shape(X_train.as_matrix())\n",
    "\n",
    "non_num_columns=['AffinityCodeId','SelectedPackage','NameOfPolicyProduct','SCID'] # Complete\n",
    "dropped_columns=['CustomerMD5Key','ReceivedDateTime','Unnamed: 0','TodayDate']\n",
    "\n",
    "\n",
    "for column in non_num_columns:\n",
    "    X_train[column]=le.fit_transform(X_train[column].as_matrix())\n",
    "\n",
    "all_columns=df.columns.tolist()\n",
    "for column_name in dropped_columns:\n",
    "    all_columns.remove(column_name)\n",
    "\n",
    "reduced_df=df[all_columns]\n",
    "\n",
    "discrete_columns=['SelectedPackage','FirstDriverMaritalStatus','CarAnnualMileage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct a dictionary\n",
    "dictionary = corpora.Dictionary(equiv_texts)\n",
    "dictionary.save('topics_alex.dict')\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in equiv_texts]\n",
    "corpora.MmCorpus.serialize('topics_alex.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corpus=corpora.MmCorpus.load('topics_alex.mm')\n",
    "mm = corpora.MmCorpus('topics_alex.mm')\n",
    "id2word = corpora.Dictionary.load('topics_alex.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_LDA=True\n",
    "n_topics=5\n",
    "if train_LDA:\n",
    "    lda = LdaMulticore(mm,id2word=id2word,num_topics=n_topics,workers=16)\n",
    "    cPickle.dump(lda,open('lda_alex_'+str(n_topics)+'_topics.p','wb'))\n",
    "else:\n",
    "    lda=cPickle.load(open('lda_alex_'+str(n_topics)+'_topics.p','rb'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DON T USE THE NEXT CELLS : GO TO lda_features_generator for features generation from topics learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_vocab(idx_doc_bow):\n",
    "    global lda,n_topics,mm\n",
    "    bow_converted=lda[mm[idx_doc_bow]]\n",
    "    local_array=[0]*(n_topics+1)\n",
    "    for feat_tuple in bow_converted:\n",
    "        local_array[feat_tuple[0]+1]=feat_tuple[1]\n",
    "    return local_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "p=Pool(10)\n",
    "\n",
    "L_indices=list(range(lenX))\n",
    "        \n",
    "# Compute the feature vector :\n",
    "a=p.map(transform_vocab,L_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cPickle.dump(a,open('sauvegarde_qui_sauve_la_vie.p','wb'))\n",
    "topic_features_df=pd.DataFrame(a)\n",
    "del a\n",
    "topic_features_df=topic_features_df[topic_features_df.columns[1:]]\n",
    "topic_features_df.columns=['lda_feature_'+str(i) for i in range(len(topic_features_df.columns))]\n",
    "\n",
    "cPickle.dump(topic_features_df,open('lda_features_'+str(n_topics)+'topics_df.csv','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#global n_topics,ldamm\n",
    "\n",
    "def dataframe2corpus(df,name_file):\n",
    "    '''\n",
    "    df : dataframe containing the data\n",
    "    name_file : name of file saved containing the dictionnary for the lda model\n",
    "    '''\n",
    "    non_num_columns=['AffinityCodeId','SelectedPackage','NameOfPolicyProduct','SCID']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for column in non_num_columns:\n",
    "        df[column]=le.fit_transform(df[column].as_matrix())\n",
    "    dropped_columns=['CustomerMD5Key','ReceivedDateTime','Unnamed: 0','TodayDate']\n",
    "    all_columns=df.columns.tolist()\n",
    "    for column_name in dropped_columns:\n",
    "        all_columns.remove(column_name)\n",
    "\n",
    "    reduced_df=df[all_columns]\n",
    "    \n",
    "    # List of column names with categorical variables\n",
    "    categ_vars=[]\n",
    "    for column in reduced_df.columns:\n",
    "        if len(np.unique(reduced_df[column]))<50:    # we don't keep columns with too many different values\n",
    "            categ_vars+=[column]\n",
    "\n",
    "# Encode the data as a bag of words to feed gensim LDA\n",
    "    equiv_texts=[]\n",
    "    all_cat=reduced_df[categ_vars].as_matrix().tolist()\n",
    "    for row in all_cat:\n",
    "        text_row=[str(categ_vars[i])+':'+str(word) for (i,word) in enumerate(row)]\n",
    "        equiv_texts+=[text_row]\n",
    "    \n",
    "    \n",
    "    #dictionary = corpora.Dictionary(equiv_texts)\n",
    "    #dictionary.save('topics_'+str(name_file)+'.dict') # dictionnary id2word\n",
    "    dictionary=corpora.Dictionary.load('topics_alex.dict')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in equiv_texts]\n",
    "    corpora.MmCorpus.serialize('topics_'+str(name_file)+'.mm', corpus) # serialized dictionnary (for fast lda)\n",
    "    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "n_topics=5\n",
    "t1=time.time()\n",
    "\n",
    "name_file='train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/lib/arraysetops.py:200: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285.659765959\n"
     ]
    }
   ],
   "source": [
    "#X_test=pd.read_csv('X_test.csv')\n",
    "X_train=pd.read_csv('X_train.csv')\n",
    "\n",
    "df=X_train\n",
    "del X_train\n",
    "\n",
    "dataframe2corpus(df,name_file)\n",
    "lenX=np.shape(df.as_matrix())[0]\n",
    "del df\n",
    "\n",
    "\n",
    "t2=time.time() # step 1 done : stored the vocabulary in the data\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285.679936171\n",
      "582.179489851\n"
     ]
    }
   ],
   "source": [
    "# step 2 : \n",
    "t2=time.time()\n",
    "\n",
    "\n",
    "lda=cPickle.load(open('lda_alex_'+str(n_topics)+'_topics.p','rb')) # load lda model trained on the big corpus\n",
    "\n",
    "mm_test = corpora.MmCorpus('topics_'+str(name_file)+'.mm')\n",
    "id2word = corpora.Dictionary.load('topics_alex.dict')\n",
    "   \n",
    "#print('updating')\n",
    "#lda.update(mm)\n",
    "#print('ended updating')    \n",
    "    \n",
    "\n",
    "\n",
    "ldamm=lda[mm_test]\n",
    "\n",
    "del mm_test,lda # saving memory\n",
    "\n",
    "L_indices=list(range(lenX)) ##########\n",
    "\n",
    "\n",
    "def transform_vocab(idx_doc_bow):\n",
    "    global n_topics,ldamm\n",
    "    bow_converted=ldamm[idx_doc_bow]\n",
    "    local_array=[0]*(n_topics+1)\n",
    "    for feat_tuple in bow_converted:\n",
    "        local_array[feat_tuple[0]+1]=feat_tuple[1]\n",
    "    return local_array\n",
    "        \n",
    "# Compute the feature vector :\n",
    "p=Pool(10)\n",
    "a=p.map(transform_vocab,L_indices)\n",
    "\n",
    "cPickle.dump(a,open('sauvegarde_qui_sauve_la_vie.p','wb'))\n",
    "topic_features_df=pd.DataFrame(a)\n",
    "del a\n",
    "topic_features_df=topic_features_df[topic_features_df.columns[1:]]\n",
    "topic_features_df.columns=['lda_feature_'+str(i) for i in range(len(topic_features_df.columns))]\n",
    "topic_features_df.to_csv('lda_features_'+str(n_topics)+'_'+str(name_file)+'_topics_df.csv',index=None)\n",
    "#compute_lda_feature(n_topics,name_file)\n",
    "t3=time.time()\n",
    "print(t2-t1)\n",
    "print(t3-t2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
